{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Matching Attack on a Pytorch Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will learn how to use ART to run a clean-label gradient matching poisoning attack on a neural network trained with Pytorch. We will be training our data on a subset of the CIFAR-10 dataset. The methods described are derived from [this paper](https://arxiv.org/abs/2009.02276) by Geiping, et. al. 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model to attack\n",
    "\n",
    "In this example, we use a RESNET50 model on the CIFAR dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 13:30:16.772687: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "module_path = os.path.abspath(os.path.join('.'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "from art.utils import load_cifar10\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "(x_train, y_train), (x_test, y_test), min_, max_ = load_cifar10()\n",
    "\n",
    "mean = np.mean(x_train,axis=(0,1,2,3))\n",
    "std = np.std(x_train,axis=(0,1,2,3))\n",
    "x_train = (x_train-mean)/(std+1e-7)\n",
    "x_test = (x_test-mean)/(std+1e-7)\n",
    "\n",
    "x_train = np.transpose(x_train, [0, 3,1,2])\n",
    "x_test = np.transpose(x_test, [0, 3,1,2])\n",
    "\n",
    "min_ = (min_-mean)/(std+1e-7)\n",
    "max_ = (max_-mean)/(std+1e-7)\n",
    "\n",
    "\n",
    "# Model from: https://github.com/kuangliu/pytorch-cifar\n",
    "# MIT License\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def resnet_18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "# Function to test the model with the test dataset and print the accuracy for the test images\n",
    "def testAccuracy(model, test_loader, max_steps=10):\n",
    "    model_was_training = model.training\n",
    "    model.eval()\n",
    "    accuracy = 0.0\n",
    "    total = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            # run the model on the test set to predict labels\n",
    "            outputs = model(images)\n",
    "            # the label with the highest energy will be our prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            accuracy += (predicted == labels).sum().item()\n",
    "    \n",
    "    # compute the accuracy over all test images\n",
    "    accuracy = (100 * accuracy / total)\n",
    "    if model_was_training:\n",
    "      model.train()\n",
    "    return(accuracy)\n",
    "\n",
    "def create_model(x_train, y_train, x_test=None, y_test=None, num_classes=10, batch_size=128, epochs=25, x_trigger=None, y_trigger=None):\n",
    "    if x_test==None or y_test==None:\n",
    "        x_test = x_train\n",
    "        y_test = y_train\n",
    "    model = resnet_18()\n",
    "\n",
    "    if x_trigger is not None:\n",
    "        assert(x_trigger.shape[0] == 1)\n",
    "        x_trigger = torch.tensor(x_trigger, dtype=torch.float32, device=device)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4, nesterov=True)\n",
    "    model.to(device)\n",
    "\n",
    "    y_train = np.argmax(y_train, axis=1)\n",
    "    x_tensor = torch.tensor(x_train, dtype=torch.float32, device=device) # transform to torch tensor\n",
    "    y_tensor = torch.tensor(y_train, dtype=torch.long, device=device)\n",
    "\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "    x_tensor_test = torch.tensor(x_test, dtype=torch.float32, device=device) # transform to torch tensor\n",
    "    y_tensor_test = torch.tensor(y_test, dtype=torch.long, device=device)\n",
    "\n",
    "    dataset_train = TensorDataset(x_tensor,y_tensor) # create your datset\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size=batch_size)\n",
    "\n",
    "    dataset_test = TensorDataset(x_tensor_test,y_tensor_test) # create your datset\n",
    "    dataloader_test = DataLoader(dataset_test, batch_size=batch_size)\n",
    "\n",
    "    iter = trange(epochs)\n",
    "    for _ in iter:\n",
    "      running_loss = 0.0\n",
    "      total = 0\n",
    "      accuracy = 0\n",
    "      for _, data in enumerate(dataloader_train, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        accuracy += (predicted == labels).sum().item()\n",
    "        running_loss += loss.item()\n",
    "      train_accuracy = (accuracy / total)\n",
    "      if x_trigger is not None:\n",
    "        y_ = model(x_trigger)\n",
    "        y_ = F.softmax(y_, dim=-1)[0]\n",
    "        output_target = y_.detach().cpu().numpy()[y_trigger]\n",
    "        iter.set_postfix({'acc': train_accuracy, 'target': output_target})\n",
    "        tqdm.tqdm.write(str(output_target))\n",
    "      else:\n",
    "        iter.set_postfix({'acc': train_accuracy})\n",
    "    test_accuracy = testAccuracy(model, dataloader_test)\n",
    "    print(\"Final test accuracy: %f\" % test_accuracy)\n",
    "\n",
    "    del x_tensor, y_tensor\n",
    "    del x_tensor_test, y_tensor_test\n",
    "    del dataset_train, dataloader_train\n",
    "    del dataset_test, dataloader_test\n",
    "\n",
    "    return model, loss_fn, optimizer\n",
    "\n",
    "\n",
    "# # model_path = \"cifar10-resnet18-pytorch.pth\"\n",
    "# model_path = \"scifar10-resnet18-pytorch.pth\"\n",
    "# if not os.path.exists(model_path):\n",
    "#     model, loss_fn, optimizer = create_model(x_train, y_train, epochs=80)\n",
    "#     torch.save(model.state_dict(), model_path)\n",
    "# else:\n",
    "#     model, loss_fn, optimizer = create_model(x_train, y_train, epochs=0)\n",
    "#     model.load_state_dict(torch.load(model_path))\n",
    "#     model.load('sp-model-pytorch-92.9.pth')\n",
    "\n",
    "model, loss_fn, optimizer = create_model(x_train, y_train, epochs=80)\n",
    "model_art = PyTorchClassifier(model, input_shape=x_train.shape[1:], loss=loss_fn, optimizer=optimizer, nb_classes=10)\n",
    "\n",
    "print(\"Model and data preparation done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, loss_fn, optimizer = create_model(x_train, y_train, epochs=0)\n",
    "model.load_state_dict(torch.load('final-model-shriti.pt'))\n",
    "model_art = PyTorchClassifier(model, input_shape=x_train.shape[1:], loss=loss_fn, optimizer=optimizer, nb_classes=10)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model_art.model.state_dict(), 'final-model-shriti.pt')\n",
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Target Image from Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from art.utils import to_categorical\n",
    "\n",
    "# # A trigger from class 0 will be classified into class 1.\n",
    "# class_source = 0\n",
    "# class_target = 1\n",
    "# index_target = np.where(y_test.argmax(axis=1)==class_source)[0][5]\n",
    "\n",
    "# # Trigger sample\n",
    "# x_trigger = x_test[index_target:index_target+1]\n",
    "# y_trigger  = to_categorical([class_target], nb_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed patch location bottom right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "\n",
    "img = Image.open('trigger_10.png')\n",
    "\n",
    "# PIL images into NumPy arrays\n",
    "numpydata = asarray(img)\n",
    "print(numpydata.shape)\n",
    "patch = np.transpose(resize(numpydata, (8,8,3)),(2,0,1))\n",
    "patch=(patch-mean)/(std+1e-7)\n",
    "K = 1000 # Number of samples to be taken from train images\n",
    "\n",
    "# A trigger from class 0 will be classified into class 1.\n",
    "class_source = 0\n",
    "class_target = 1\n",
    "\n",
    "# index_target = np.where(y_test.argmax(axis=1)==class_source)[0][5]\n",
    "# Here we work on train data\n",
    "indices_target = np.where(y_train.argmax(axis=1)==class_source)[0][0:K]\n",
    "x_trigger = x_train[indices_target]\n",
    "print(x_trigger.shape)\n",
    "print(\"shape of patch\",patch.shape)\n",
    "x_trigger[:,:,-8:,-8:] = patch\n",
    "y_trigger = to_categorical([class_target], num_classes=10)\n",
    "y_trigger = np.tile(y_trigger, (len(indices_target), 1))\n",
    "\n",
    "# This is to make sure, that the train images are not being changed\n",
    "# plt.figure(1)\n",
    "# plt.imshow(x_trigger[1])\n",
    "# plt.figure(2)\n",
    "# plt.imshow(x_train[indices_target[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[indices_target[1]].transpose([1,2,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_trigger[1].transpose([1,2,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Patch Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.utils import to_categorical\n",
    "# from PIL import Image\n",
    "# from numpy import asarray\n",
    "# import matplotlib.pyplot as plt\n",
    "# from skimage.transform import resize\n",
    "# import random\n",
    "\n",
    "# img = Image.open('trigger_10.png')\n",
    "\n",
    "# # PIL images into NumPy arrays\n",
    "# numpydata = asarray(img)\n",
    "# print(numpydata.shape)\n",
    "# patch = np.transpose(resize(numpydata, (8,8,3)),(2,0,1))\n",
    "# K = 1000 # Number of samples to be taken from train images\n",
    "\n",
    "# # A trigger from class 0 will be classified into class 1.\n",
    "# class_source = 0\n",
    "# class_target = 1\n",
    "\n",
    "\n",
    "\n",
    "# # index_target = np.where(y_test.argmax(axis=1)==class_source)[0][5]\n",
    "# # Here we work on train data\n",
    "# indices_target = np.where(y_train.argmax(axis=1)==class_source)[0][0:K]\n",
    "# x_trigger = x_train[indices_target]\n",
    "# print(x_trigger.shape)\n",
    "# print(\"shape of patch\",patch.shape)\n",
    "\n",
    "# ######### APPLYING RANDOM PATCH LOCATION STRATEGY ########\n",
    "# for x in x_trigger:\n",
    "#     x_cord = random.randint(0, 24)\n",
    "#     y_cord = random.randint(0, 24)\n",
    "#     x[:,x_cord:x_cord+8,y_cord:y_cord+8]=patch\n",
    "\n",
    "# y_trigger = to_categorical([class_target], num_classes=10)\n",
    "# y_trigger = np.tile(y_trigger, (len(indices_target), 1))\n",
    "# # plt.imshow(x_trigger[1].transpose([1,2,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poison Training Images to Misclassify the Trigger Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.attacks.poisoning.gradient_matching_attack import GradientMatchingAttack\n",
    "\n",
    "\n",
    "######### Sleeper agent values ########\n",
    "# RESNET18\n",
    "# 16/255 bounded by l-infinity\n",
    "# 1% of training images\n",
    "\n",
    "\n",
    "\n",
    "# epsilson = 0.01/(std+1e-7)\n",
    "factor = 16/255\n",
    "epsilson = factor * (max_-min_)\n",
    "print(epsilson)\n",
    "attack = GradientMatchingAttack(model_art,\n",
    "        percent_poison=0.5,\n",
    "        max_trials=1,\n",
    "        max_epochs=500,\n",
    "        clip_values=(min_,max_),\n",
    "        learning_rate_schedule=(np.array([1e-1, 1e-2, 1e-3, 1e-4, 1e-5]), [250, 350, 400, 430, 460]),\n",
    "        epsilon=epsilson,\n",
    "        verbose=1)\n",
    "\n",
    "x_poison, y_poison = attack.poison(x_trigger, y_trigger, x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of the trigger, an original sample, and the poisoned sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_trigger[0].transpose([1,2,0])*(std+1e-7)+mean)\n",
    "plt.title('Trigger image')\n",
    "plt.show()\n",
    "\n",
    "index_poisoned_example = np.where([np.any(p!=o) for (p,o) in zip(x_poison,x_train)])[0]\n",
    "plt.imshow(x_train[index_poisoned_example[0]].transpose([1,2,0])*(std+1e-7)+mean)\n",
    "plt.title('Original image')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(x_poison[index_poisoned_example[0]].transpose([1,2,0])*(std+1e-7)+mean)\n",
    "plt.title('Poisoned image')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Poison Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These attacks allow adversaries who can poison your dataset the ability to mislabel any particular target instance of their choosing without manipulating labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_poison.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_poison.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.utils import to_categorical\n",
    "# from PIL import Image\n",
    "# from numpy import asarray\n",
    "# import matplotlib.pyplot as plt\n",
    "# from skimage.transform import resize\n",
    "\n",
    "# img = Image.open('trigger_10.png')\n",
    "\n",
    "# # PIL images into NumPy arrays\n",
    "# numpydata = asarray(img)\n",
    "# print(numpydata.shape)\n",
    "# patch = np.transpose(resize(numpydata, (8,8,3)),(2,0,1))\n",
    "#patch=(patch-mean)/(std+1e-7)\n",
    "# K = 1000 # Number of samples to be taken from train images\n",
    "\n",
    "# # A trigger from class 0 will be classified into class 1.\n",
    "# class_source = 0\n",
    "# class_target = 1\n",
    "\n",
    "# # index_target = np.where(y_test.argmax(axis=1)==class_source)[0][5]\n",
    "# # Here we work on train data\n",
    "# indices_target = np.where(y_train.argmax(axis=1)==class_source)[0][0:K]\n",
    "# x_trigger = x_train[indices_target]\n",
    "# print(x_trigger.shape)\n",
    "# print(\"shape of patch\",patch.shape)\n",
    "# x_trigger[:,:,-8:,-8:] = patch\n",
    "# y_trigger = to_categorical([class_target], num_classes=10)\n",
    "# y_trigger = np.tile(y_trigger, (len(indices_target), 1))\n",
    "\n",
    "# # This is to make sure, that the train images are not being changed\n",
    "# # plt.figure(1)\n",
    "# # plt.imshow(x_trigger[1])\n",
    "# # plt.figure(2)\n",
    "# # plt.imshow(x_train[indices_target[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A trigger from class 0 will be classified into class 1.\n",
    "class_source = 0\n",
    "class_target = 1\n",
    "\n",
    "# index_target = np.where(y_test.argmax(axis=1)==class_source)[0][5]\n",
    "# Here we work on train data\n",
    "# indices_target = np.where(y_test.argmax(axis=1)==class_source)[0][0:]\n",
    "indices_target = np.where(y_test.argmax(axis=1)==class_source)[0][0:]\n",
    "# x_trigger = x_test[index_target:index_target+1]\n",
    "x_trigger = x_test[indices_target]\n",
    "print(x_trigger.shape)\n",
    "x_trigger[:,:,-8:,-8:] = patch\n",
    "y_trigger = to_categorical([class_target], num_classes=10)\n",
    "y_trigger = np.tile(y_trigger, (len(indices_target), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_poisoned = create_model(x_poison, y_poison, epochs=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=(patch-mean)/(std+1e-7)\n",
    "np.min(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the magnitude of perturbations if  ||perturbation||< epsilon and each perturbation >0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max((x_train - x_poison)[18496])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate success rate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_source = 0\n",
    "# class_target = 1\n",
    "# indices_target = np.where(y_test.argmax(axis=1)==class_source)[0][0:]\n",
    "# x_trigger = x_test[indices_target]\n",
    "# for x in x_trigger:\n",
    "#     x_cord = random.randint(0, 24)\n",
    "#     y_cord = random.randint(0, 24)\n",
    "#     x[:,x_cord:x_cord+8,y_cord:y_cord+8]=patch\n",
    "\n",
    "# y_trigger = to_categorical([class_target], num_classes=10)\n",
    "# y_trigger = np.tile(y_trigger, (len(indices_target), 1))\n",
    "# # plt.imshow(x_trigger[1].transpose([1,2,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = model_poisoned[0](torch.tensor(x_trigger, device=device, dtype=torch.float)).detach().cpu().numpy()\n",
    "\n",
    "print(\"y_trigger:\", y_trigger)\n",
    "print(\"y_:\", y_)\n",
    "\n",
    "if np.argmax(y_trigger) == np.argmax(y_):\n",
    "    print(\"Poisoning was successful.\")\n",
    "else:\n",
    "    print(\"Poisoning failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape of y_trigger\",y_trigger.shape)\n",
    "print(\"shaoe of y_\",y_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Success Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = (np.argmax(y_trigger,axis=1) == np.argmax(y_,axis=1)).sum()\n",
    "\n",
    "print(\"success rate:\",acc/len(y_)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_test[indices_target[1]].transpose([1,2,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_trigger[indices_target[0]].transpose([1,2,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_trigger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test[indices_target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0efaf3e5c0b4bd1ede177191899ec2ef4ee13bfdededa3ba02bc2fc62340f8fa"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
